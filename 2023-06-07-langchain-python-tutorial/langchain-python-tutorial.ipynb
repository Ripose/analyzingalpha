{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0766b189",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8029e5be",
   "metadata": {},
   "source": [
    "## Prerequisite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a63e807",
   "metadata": {},
   "source": [
    "### Installation \n",
    "\n",
    "! pip install langchain\n",
    "\n",
    "Or\n",
    "\n",
    "! conda install langchain -c conda-forge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c40906a",
   "metadata": {},
   "source": [
    "## 1.  Getting Started with LangChain Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d81b1d9",
   "metadata": {},
   "source": [
    "### Calling an LLM with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a4c86e",
   "metadata": {},
   "source": [
    "! pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "322128d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "api_key = os.getenv('OpenAI_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b7a2881",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(openai_api_key = api_key, temperature=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef7724ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Beachside Scoops\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Suggest me a good name for an ice cream parlour that is located on a beach!\"\n",
    "print(llm(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3b89dd",
   "metadata": {},
   "source": [
    "## LangChain Modules \n",
    "\n",
    "* Models\n",
    "* Prompts\n",
    "* Chains\n",
    "* Agents\n",
    "* Memory\n",
    "* Document Loaders\n",
    "* Indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4266000b",
   "metadata": {},
   "source": [
    "## 2. Models\n",
    "\n",
    "* Large Language Models (LLMs)\n",
    "* Chat Models\n",
    "* Text Embedding Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dc7534",
   "metadata": {},
   "source": [
    "### a. Large Language Models (LLMs)\n",
    "\n",
    "**Note:** check this link to see all supported language models\n",
    "\n",
    "https://python.langchain.com/en/latest/modules/models/llms/integrations.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405a1c60",
   "metadata": {},
   "source": [
    "#### OpenAI Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4231b1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(openai_api_key = api_key,\n",
    "             model_name=\"text-davinci-003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b656102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Q: What has a bed but never sleeps, and runs but never walks?\n",
      "A: A river.\n"
     ]
    }
   ],
   "source": [
    "print(llm(\"Can you tell me a riddle about water?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eed1cf6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_result = llm.generate([\"Write a poem about hills\", \"Tell me a riddle about oranges\"])\n",
    "len(llm_result.generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0b8676b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "On a hillside, I stand\n",
      "The lush grass, so green\n",
      "The wind caresses my face\n",
      "As I take in the view\n",
      "\n",
      "The hills are a paradise\n",
      "Where I am free to roam\n",
      "I hike up the craggy trails\n",
      "My heart and my soul to be shown\n",
      "\n",
      "The sky is so bright and clear\n",
      "The sun, a brilliant light\n",
      "The hills are a never-ending dream\n",
      "Filled with beauty and delight\n",
      "\n",
      "The birds sing a sweet melody\n",
      "As I wander on my way\n",
      "In the hills I'm at peace\n",
      "And I love it there, I say!\n"
     ]
    }
   ],
   "source": [
    "print(llm_result.generations[0][0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84009c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Q: What has an orange outside and a white inside?\n",
      "A: An orange peel!\n"
     ]
    }
   ],
   "source": [
    "print(llm_result.generations[1][0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc26a8b",
   "metadata": {},
   "source": [
    "#### Transformers Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26c7c5fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b07562b5fff14c96928a247576fd7b2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c5e0e0fe267401a96008dec89c2d2cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.llms import CTransformers\n",
    "\n",
    "llm = CTransformers(model='marella/gpt-2-ggml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07f1a352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sunday. I will be in the city for a few days and then fly back home.\"\n",
      ", which is not yet confirmed by FIFA but was announced at an event held last week with Brazilian President Dilma Rousseff , said that she would meet her counterpart next month \"to discuss ways of improving relations between Brazil and Europe\".\n"
     ]
    }
   ],
   "source": [
    "print(llm(\"I am flying to Lisbon on\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d278a3b2",
   "metadata": {},
   "source": [
    "### b. Chat Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28c2289",
   "metadata": {},
   "source": [
    "#### Generating a Single Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35b8a977",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from langchain.schema import (\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "403d796d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(openai_api_key = api_key, \n",
    "                  temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f60006a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'aime jouer au tennis.\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_message = \"Translate from English to Frech: I love playing Tennis\"\n",
    "chat([HumanMessage(content = human_message)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "413c5731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The 11th FIFA World Cup was held in Argentina in 1978. The player of the tournament was awarded to Mario Kempes from Argentina. He scored six goals in the tournament, including two in the final against the Netherlands, which Argentina won 3-1.', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are a football historian.\"),\n",
    "    HumanMessage(content=\" Who won the player of the tournament in the 11th Fifa World Cup?\")\n",
    "]\n",
    "chat(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4d2d84",
   "metadata": {},
   "source": [
    "#### Generating Batch Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "301b5ce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[ChatGeneration(text='The 11th FIFA World Cup was held in Argentina in 1978. The player of the tournament was awarded to Mario Kempes from Argentina. He scored six goals in the tournament, including two in the final against the Netherlands, which Argentina won 3-1.', generation_info=None, message=AIMessage(content='The 11th FIFA World Cup was held in Argentina in 1978. The player of the tournament was awarded to Mario Kempes from Argentina. He scored six goals in the tournament, including two in the final against the Netherlands, which Argentina won 3-1.', additional_kwargs={}, example=False))], [ChatGeneration(text=\"Sure, here's a 7-step recipe to prepare a pizza:\\n\\nIngredients:\\n- 1 pizza dough\\n- 1/2 cup pizza sauce\\n- 2 cups shredded mozzarella cheese\\n- 1/4 cup grated Parmesan cheese\\n- 1/4 cup chopped fresh basil\\n- 1/4 cup sliced pepperoni\\n- 1/4 cup sliced black olives\\n\\nInstructions:\\n\\n1. Preheat your oven to 450°F (230°C).\\n\\n2. Roll out the pizza dough on a floured surface until it's about 12 inches in diameter.\\n\\n3. Spread the pizza sauce evenly over the dough, leaving about 1/2 inch of space around the edges.\\n\\n4. Sprinkle the shredded mozzarella cheese over the sauce, followed by the grated Parmesan cheese.\\n\\n5. Add the chopped fresh basil, sliced pepperoni, and sliced black olives on top of the cheese.\\n\\n6. Place the pizza on a baking sheet or pizza stone and bake for 10-12 minutes, or until the crust is golden brown and the cheese is melted and bubbly.\\n\\n7. Remove the pizza from the oven and let it cool for a few minutes before slicing and serving. Enjoy your delicious homemade pizza!\", generation_info=None, message=AIMessage(content=\"Sure, here's a 7-step recipe to prepare a pizza:\\n\\nIngredients:\\n- 1 pizza dough\\n- 1/2 cup pizza sauce\\n- 2 cups shredded mozzarella cheese\\n- 1/4 cup grated Parmesan cheese\\n- 1/4 cup chopped fresh basil\\n- 1/4 cup sliced pepperoni\\n- 1/4 cup sliced black olives\\n\\nInstructions:\\n\\n1. Preheat your oven to 450°F (230°C).\\n\\n2. Roll out the pizza dough on a floured surface until it's about 12 inches in diameter.\\n\\n3. Spread the pizza sauce evenly over the dough, leaving about 1/2 inch of space around the edges.\\n\\n4. Sprinkle the shredded mozzarella cheese over the sauce, followed by the grated Parmesan cheese.\\n\\n5. Add the chopped fresh basil, sliced pepperoni, and sliced black olives on top of the cheese.\\n\\n6. Place the pizza on a baking sheet or pizza stone and bake for 10-12 minutes, or until the crust is golden brown and the cheese is melted and bubbly.\\n\\n7. Remove the pizza from the oven and let it cool for a few minutes before slicing and serving. Enjoy your delicious homemade pizza!\", additional_kwargs={}, example=False))]], llm_output={'token_usage': {'prompt_tokens': 67, 'completion_tokens': 307, 'total_tokens': 374}, 'model_name': 'gpt-3.5-turbo'})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_messages = [\n",
    "    [\n",
    "        SystemMessage(content=\"You are a football historian.\"),\n",
    "        HumanMessage(content=\" Who won the player of the tournament in the 11th Fifa World Cup?\")\n",
    "    ],\n",
    "    [\n",
    "        SystemMessage(content=\"You are a Pizza chef.\"),\n",
    "        HumanMessage(content=\"Give me the 7-step recipe to prepare a pizza.\")\n",
    "    ],\n",
    "]\n",
    "result = chat.generate(batch_messages)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19f2abed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 11th FIFA World Cup was held in Argentina in 1978. The player of the tournament was awarded to Mario Kempes from Argentina. He scored six goals in the tournament, including two in the final against the Netherlands, which Argentina won 3-1.\n"
     ]
    }
   ],
   "source": [
    "print(result.generations[0][0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa0ea343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a 7-step recipe to prepare a pizza:\n",
      "\n",
      "Ingredients:\n",
      "- 1 pizza dough\n",
      "- 1/2 cup pizza sauce\n",
      "- 2 cups shredded mozzarella cheese\n",
      "- 1/4 cup grated Parmesan cheese\n",
      "- 1/4 cup chopped fresh basil\n",
      "- 1/4 cup sliced pepperoni\n",
      "- 1/4 cup sliced black olives\n",
      "\n",
      "Instructions:\n",
      "\n",
      "1. Preheat your oven to 450°F (230°C).\n",
      "\n",
      "2. Roll out the pizza dough on a floured surface until it's about 12 inches in diameter.\n",
      "\n",
      "3. Spread the pizza sauce evenly over the dough, leaving about 1/2 inch of space around the edges.\n",
      "\n",
      "4. Sprinkle the shredded mozzarella cheese over the sauce, followed by the grated Parmesan cheese.\n",
      "\n",
      "5. Add the chopped fresh basil, sliced pepperoni, and sliced black olives on top of the cheese.\n",
      "\n",
      "6. Place the pizza on a baking sheet or pizza stone and bake for 10-12 minutes, or until the crust is golden brown and the cheese is melted and bubbly.\n",
      "\n",
      "7. Remove the pizza from the oven and let it cool for a few minutes before slicing and serving. Enjoy your delicious homemade pizza!\n"
     ]
    }
   ],
   "source": [
    "print(result.generations[1][0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ba9b56",
   "metadata": {},
   "source": [
    "### c. Text Embedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8d8c29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key = api_key)\n",
    "\n",
    "text = \"Paris the capital of france and is famous for its wines and perfumes\"\n",
    "embeddings_result = embeddings.embed_query(text)\n",
    "\n",
    "print(len(embeddings_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1307315d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Embeddings: 3\n"
     ]
    }
   ],
   "source": [
    "text = [\"Paris the capital of france and is famous for its wines and perfumes\",\n",
    "        \"London is the Capital of England\",\n",
    "        \"Tokyo is the Capital of Japan\"\n",
    "       ]\n",
    "\n",
    "embeddings_result = embeddings.embed_documents(text)\n",
    "\n",
    "print(\"Total Embeddings:\", len(embeddings_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf5663c",
   "metadata": {},
   "source": [
    "## 3. Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b4d7e2",
   "metadata": {},
   "source": [
    "### a. LLM Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "550da7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"Can you tell me a riddle about {object} with its answer?\" \n",
    "prompt = PromptTemplate(\n",
    "    template = template,\n",
    "    input_variables=[\"object\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7cbfdcb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can you tell me a riddle about ice with its answer?\n"
     ]
    }
   ],
   "source": [
    "prompt = prompt.format(object=\"ice\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c06b88ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Q: What goes up but never comes down?\n",
      "A: The temperature of ice!\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(openai_api_key = api_key)\n",
    "print(llm(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14816388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## install this package before running this command\n",
    "## pip install tiktoken\n",
    "llm.get_num_tokens(\"This is a terrible Joke!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf459076",
   "metadata": {},
   "source": [
    "### b. Chat Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98af393f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ddf0486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are a Tennis historian.', additional_kwargs={}), HumanMessage(content='Who won the Australian Open in 2015', additional_kwargs={}, example=False)]\n"
     ]
    }
   ],
   "source": [
    "system_template = \"You are a {sports} historian.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "\n",
    "human_template = \"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "chat_prompt = chat_prompt.format_prompt(sports=\"Tennis\", \n",
    "                                        text=\"Who won the Australian Open in 2015\").to_messages()\n",
    "\n",
    "print(chat_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ea42868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"The men's singles champion of the Australian Open in 2015 was Novak Djokovic from Serbia, and the women's singles champion was Serena Williams from the United States.\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(chat_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058c558b",
   "metadata": {},
   "source": [
    "## 4. Chains\n",
    "\n",
    "* Simple LLM Chain\n",
    "* Creating Sequential Chains\n",
    "* Creating a Custom Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2193e09e",
   "metadata": {},
   "source": [
    "### a. Simple LLM Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a76c9f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(openai_api_key = api_key,\n",
    "              temperature=0.9)\n",
    "             \n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"object\", \"location\"],\n",
    "    template=\"Suggest me a good name for {object} shop, located on {location}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "770baaaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " side\n",
      "\n",
      "SeaLux Boutique\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "print(chain.run({\n",
    "    'object': \"clothes\",\n",
    "    'location': \"beach\"\n",
    "    }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b269c4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Australian Open Men's Singles in 2015 was won by Novak Djokovic of Serbia, and the Women's Singles was won by Serena Williams of the United States.\n"
     ]
    }
   ],
   "source": [
    "system_template = \"You are a {sports} historian.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "\n",
    "human_template = \"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "chat = ChatOpenAI(openai_api_key = api_key,\n",
    "                  temperature=0)\n",
    "\n",
    "chain = LLMChain(llm=chat, prompt=chat_prompt_template)\n",
    "print(chain.run({\n",
    "    'sports': \"Tennis\",\n",
    "    'text': \"Who won Austrialian Open in 2015?\"\n",
    "    }))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49efe4d",
   "metadata": {},
   "source": [
    "### b. Sequentionl Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a1b01970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "\n",
      "Sand and Surf Apparel\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m: \"Where the Coast Meets Your Closet!\"\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      ": \"Where the Coast Meets Your Closet!\"\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "prompt1 = PromptTemplate(\n",
    "    input_variables=[\"location\"],\n",
    "    template=\"Suggest me a good name for a clothing shop, located on {location}\",\n",
    ")\n",
    "\n",
    "chain1 = LLMChain(llm=llm, prompt=prompt1)\n",
    "\n",
    "prompt2 = PromptTemplate(\n",
    "    input_variables=[\"location\"],\n",
    "    template=\"Write a catchy tag line for a clothing shop, located on {location}\",\n",
    ")\n",
    "\n",
    "chain2 = LLMChain(llm=llm, prompt=prompt2)\n",
    "\n",
    "overall_chain = SimpleSequentialChain(chains=[chain1, chain2], verbose=True)\n",
    "\n",
    "# Run the chain specifying only the input variable for the first chain.\n",
    "print(overall_chain.run(\"beach\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be199b94",
   "metadata": {},
   "source": [
    "### c. Custom Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e3acb3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "from langchain.chains.base import Chain\n",
    "\n",
    "class ConcatenateChain(Chain):\n",
    "    chain_1: LLMChain\n",
    "    chain_2: LLMChain\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        # Union of the input keys of the two chains.\n",
    "        all_input_vars = set(self.chain_1.input_keys).union(set(self.chain_2.input_keys))\n",
    "        return list(all_input_vars)\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        return ['concat_output']\n",
    "\n",
    "    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\n",
    "        output_1 = self.chain_1.run(inputs)\n",
    "        output_2 = self.chain_2.run(inputs)\n",
    "        return {'concat_output': output_1 + output_2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7df5d292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Coastal Chic Boutique\n",
      "\n",
      "\"At the beach, we've got the look!\"\n"
     ]
    }
   ],
   "source": [
    "overall_chain = ConcatenateChain(chain_1=chain1, chain_2=chain2)\n",
    "print(overall_chain.run(\"beach\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828e93aa",
   "metadata": {},
   "source": [
    "## 5. Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f3543289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools, initialize_agent, AgentType\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key = api_key,\n",
    "                 temperature=0.0)\n",
    "tools = load_tools(\n",
    "    [\"arxiv\"], \n",
    ")\n",
    "\n",
    "agent_chain = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8ee568aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use Arxiv to search for the paper.\n",
      "Action: Arxiv\n",
      "Action Input: \"2303.15056\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPublished: 2023-03-27\n",
      "Title: ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks\n",
      "Authors: Fabrizio Gilardi, Meysam Alizadeh, Maël Kubli\n",
      "Summary: Many NLP applications require manual data annotations for a variety of tasks,\n",
      "notably to train classifiers or evaluate the performance of unsupervised\n",
      "models. Depending on the size and degree of complexity, the tasks may be\n",
      "conducted by crowd-workers on platforms such as MTurk as well as trained\n",
      "annotators, such as research assistants. Using a sample of 2,382 tweets, we\n",
      "demonstrate that ChatGPT outperforms crowd-workers for several annotation\n",
      "tasks, including relevance, stance, topics, and frames detection. Specifically,\n",
      "the zero-shot accuracy of ChatGPT exceeds that of crowd-workers for four out of\n",
      "five tasks, while ChatGPT's intercoder agreement exceeds that of both\n",
      "crowd-workers and trained annotators for all tasks. Moreover, the\n",
      "per-annotation cost of ChatGPT is less than $0.003 -- about twenty times\n",
      "cheaper than MTurk. These results show the potential of large language models\n",
      "to drastically increase the efficiency of text classification.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe paper is about how ChatGPT outperforms crowd-workers for text-annotation tasks.\n",
      "Final Answer: The paper with ID 2303.15056 is about how ChatGPT outperforms crowd-workers for text-annotation tasks.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The paper with ID 2303.15056 is about how ChatGPT outperforms crowd-workers for text-annotation tasks.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_chain.run(\n",
    "    \"What's the paper 2303.15056 about?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f5ab12f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import create_pandas_dataframe_agent\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r'D:\\Datasets\\titanic_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b162b35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to calculate the mean of the 'Fare' column in the dataframe.\n",
      "Action: python_repl_ast\n",
      "Action Input: df['Fare'].mean()\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m32.2042079685746\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the average Fare is 32.20.\n",
      "Final Answer: 32.20\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'32.20'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = create_pandas_dataframe_agent(llm, df, verbose=True)\n",
    "agent.run(\"What is the average Fare?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bb2c1582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to filter the dataframe to only include male passengers with age greater than 50, and then count the number of rows.\n",
      "Action: python_repl_ast\n",
      "Action Input:\n",
      "```\n",
      "len(df[(df['Sex'] == 'male') & (df['Age'] > 50)])\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m47\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer.\n",
      "Final Answer: There are 47 male passengers with age greater than 50.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'There are 47 male passengers with age greater than 50.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"Count the male pessengers with age greater than 50.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792d0e42",
   "metadata": {},
   "source": [
    "## 6. Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a3fc7d",
   "metadata": {},
   "source": [
    "### a. Conversations with Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0e41282f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "conversation.predict(input=\"Hi there.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8816c37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there.\n",
      "AI: Hello! How can I assist you today?\n",
      "Human: I have a question about Pizza\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Sure thing! What would you like to know about pizza? There are so many different types and toppings to choose from. Would you like information on the history of pizza, how it's made, or perhaps some recommendations for the best pizza places in your area?\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"I have a question about Pizza\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "76bd7b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there.\n",
      "AI: Hello! How can I assist you today?\n",
      "Human: I have a question about Pizza\n",
      "AI: Sure thing! What would you like to know about pizza? There are so many different types and toppings to choose from. Would you like information on the history of pizza, how it's made, or perhaps some recommendations for the best pizza places in your area?\n",
      "Human: Can I make it without an Oven?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Yes, you can make pizza without an oven! There are several methods you can use, such as using a stovetop, a grill, or even a microwave. One popular method is to use a cast iron skillet on the stovetop to cook the pizza. You can also use a pizza stone on a grill or even a toaster oven. It may take some experimentation to find the method that works best for you, but it's definitely possible to make delicious pizza without an oven.\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Can I make it without an Oven?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd08819",
   "metadata": {},
   "source": [
    "### b. Using Saved Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5ff87f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: hi!\\nAI: whats up?\\nHuman: I want to know something about Pizza\\nAI: Sure, what do you want to know?'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "memory.chat_memory.add_user_message(\"hi!\")\n",
    "memory.chat_memory.add_ai_message(\"whats up?\")\n",
    "\n",
    "memory.chat_memory.add_user_message(\"I want to know something about Pizza\")\n",
    "memory.chat_memory.add_ai_message(\"Sure, what do you want to know?\")\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7989a427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: hi!\n",
      "AI: whats up?\n",
      "Human: I want to know something about Pizza\n",
      "AI: Sure, what do you want to know?\n",
      "Human: Can I make it in Oven?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Yes, you can definitely make pizza in an oven. In fact, most pizza recipes are designed to be cooked in an oven. The key is to preheat your oven to the right temperature and use a baking sheet or pizza stone to ensure even cooking. Would you like me to look up some specific pizza recipes for you?'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key = api_key,\n",
    "                 temperature=0.0)\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=memory,\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "conversation.predict(input=\"Can I make it in Oven?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c90355",
   "metadata": {},
   "source": [
    "## 7. Indexes and Document Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fba9b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a18191a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(r\"D:\\Datasets\\207416.pdf\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8cf4bd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6438c996",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "db = Chroma.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "583dd6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ef212458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm, chain_type=\"stuff\", retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "100c38cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The value of total non-current assets is Rs. 21,789,510 (in thousands) as at March 31, 2023.'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Give me the value of total non current assets\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5450dd4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As per the Condensed Interim Statement of Financial Position of Indus Motor Company Limited as at March 31, 2023, the value of Long-term loans and advances is 62,884 Rupees in '000.\""
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Give me the value of Long-term loans and advances\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "52947969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the given document, we can calculate the following financial ratios:\n",
      "\n",
      "1. Gross Profit Margin: Gross Profit Margin can be calculated by dividing the Gross Profit by the Revenue from Contracts with Customers. \n",
      "\n",
      "Gross Profit Margin = (Gross Profit / Revenue from Contracts with Customers) x 100\n",
      "\n",
      "For the nine months ended March 31, 2023, the Gross Profit Margin is:\n",
      "\n",
      "Gross Profit Margin = (196,984 / 135,032,743) x 100 = 0.15%\n",
      "\n",
      "2. Net Profit Margin: Net Profit Margin can be calculated by dividing the Net Profit after Taxation by the Revenue from Contracts with Customers.\n",
      "\n",
      "Net Profit Margin = (Net Profit after Taxation / Revenue from Contracts with Customers) x 100\n",
      "\n",
      "For the nine months ended March 31, 2023, the Net Profit Margin is:\n",
      "\n",
      "Net Profit Margin = (5,843,965 / 135,032,743) x 100 = 4.32%\n",
      "\n",
      "3. Return on Equity (ROE): Return on Equity can be calculated by dividing the Net Profit after Taxation by the Shareholders' Equity.\n",
      "\n",
      "ROE = (Net Profit after Taxation / Shareholders' Equity) x 100\n",
      "\n",
      "The Shareholders' Equity is not given in the document, so we cannot calculate ROE.\n",
      "\n",
      "4. Current Ratio: Current Ratio can be calculated by dividing the Current Assets by the Current Liabilities.\n",
      "\n",
      "Current Ratio = Current Assets / Current Liabilities\n",
      "\n",
      "For the nine months ended March 31, 2023, the Current Ratio is:\n",
      "\n",
      "Current Ratio = (22,386,274 / 87,449,947) = 0.26\n",
      "\n",
      "5. Debt-to-Equity Ratio: Debt-to-Equity Ratio can be calculated by dividing the Total Liabilities by the Shareholders' Equity.\n",
      "\n",
      "Debt-to-Equity Ratio = Total Liabilities / Shareholders' Equity\n",
      "\n",
      "The Shareholders' Equity is not given in the document, so we cannot calculate Debt-to-Equity Ratio.\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"Which financial rations can you calculate from the document? \n",
    "Give values with explanation for the rations that you can calculate.\"\"\"\n",
    "print(qa.run(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd183752",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
